---
title: St 503 HW 4
author: Robin Baldeo
output: html_document
---


```{r message = F, error=FALSE, warning=F,echo = F}
library("faraway")
library("quantreg")
library("MASS")
library("splines")
```


###Question 8.1

####(a)
```{r }
mod<- lm(Lab~Field, data = pipeline)
summary(mod)

plot(mod$fitted.values, mod$residuals,xlab = "fitted", ylab = "residuals" ,col = "Red", main = "Lab~Field")
abline(h=0)

```

From the plot we see that the constant variance assumption has been violated because of the clear mega phone shape. 

####(b)
```{r}
i <- order(pipeline$Field) 
npipe <- pipeline[i,] 
ff <- gl(12,9)[-108] 
meanfield <- unlist(lapply(split(npipe$Field,ff),mean)) 
varlab <- unlist(lapply(split(npipe$Lab,ff),var))

#not removing the last obs from varlab and meanfield
modb<-lm(I(log(varlab))~I(log(meanfield)))
summary(modb)

#removing the last obs from both varlab and meanfield
modb<-lm(I(log(varlab[-length(varlab)]))~I(log(meanfield[-length(meanfield)])))
summary(modb)
```

After removing the last points as recommended by the book, the `a0` is `r modb$coeff[1]` and `a1` is `r modb$coeff[2]`

####(c)

```{r}
# applying log to both lab and field appears to correct the non cocnstant varaince voilation
modc<- lm(I(log(Lab))~I(log(Field)), data = pipeline)
summary(modc)

#plot of the log(lab)~log(field)
plot(modc$fitted.values, modc$residuals,xlab = "fitted", ylab = "residuals" ,col = "blue", main="log(lab)~log(field)")
abline(h=0)

```

Applying log to both the predictor and response appears to correct the volition of constant variance. 


###Question 8.3

```{r}
#head(salmonella)

mod_s<-lm(colonies~log(dose +1), data=salmonella)
summary(mod_s)

mod_c<-lm(colonies~factor(log(dose +1)), data=salmonella)
summary(mod_c)

anova(mod_s, mod_c)

```

Based on the `p-value` > .05  from the ANOVA test we fail to reject the null. As a result, a linear model is satisfactory. 


###question 8.5

####(a)

```{r}
#a
#least square method

ls<- lm(stack.loss~., data = stackloss)
summary(ls)


#Checking for influential points, then looking at the 5 most influential points
halfnorm(cooks.distance(ls), 5, names(cooks.distance(ls)), main="Plot of five most influential points")
```


####(b)

```{r}
#lad method

lad<- rq(stack.loss~., data = stackloss)
summary(lad)
```

####(c)

```{r}
#hubert method

hub<- rlm(stack.loss~., data = stackloss)
summary(hub)
```


####(d) 

```{r}
#additional part added
# lad weights
w_lad<- 1/abs(lad$residuals)

#hubert weight
w_hub<- hub$w

#plot of the weights of lad and hubert method
plot(c(1:21), w_lad, ylim = c(0, 1.4), pch = 19, col = "red", xlab = "points", ylab = "weight values", main= "Plot of the weights from lad and hubert method")
lines(c(1:21), w_hub, col="blue", type ="o", lty = 0 , pch = 22)
nam<- c("red", "blue")
legend("topleft", legend = c("lad", "weight"),col=nam, pch=c(19,22))
```

From the halfnorm plot in part a, we see that the top 5 influential points are 21, 1,4,3, and 17. Now with the overlay plot of the weights of LAD and Hubert, we see the weight of  most of these points are reduced in relation to the other points. With LAD, 1,3,4,and 21 weights are reduced. While with the Hubert 3,4, and 21 are assigned a lower weight. Overall these two methods prove that they are not as sensitive to the influence points as with least square and are therefore robust. 

####Comparison

Compare the results. Now use diagnostic methods to detect any outliers or influential points. Remove these points and then use least squares. Compare the results.

```{r}
#based on halfnorm point 21, 1 , 4, 3 and 17 appear to exert the top five influence on the model.
#removing these points

#
stackloss_n<- stackloss[c(-21,-1, -4, -3,-17),]

#

ls_n<- lm(stack.loss~., data = stackloss_n)
summary(ls_n)
```

When I remove the top 5 influence points 21,1,4,3, and 17. Our new modified least square parameter coefficients resemble the LAD method . This is not a surprise because the lad method down weighted 4 of the 5 influence points detected by the halfnorm. However, with Hubert method, since 3 or the 5 points are down weighted our estimate coefficients were not a close match, but when I remove just the 3 points down weighted by Hubert our coefficients were every close. This also confirms that LAD and Hubert are not sensitive to influence points as the least square. 


###question 9.3

```{r}
#question 9.3

#head(ozone)

mod9<- lm(O3~temp+humidity+ibh, data= ozone)

summary(mod9)

plot(mod9$fitted.values, mod9$residuals)
abline(h=0, col= "red")

#the constant variance assumption appears violated


qqnorm(mod9$residuals)
qqline(mod9$residuals)

boxcox(mod9, plotit = T, lambda=seq(-1,1, by = .1))

#using lambda of of .25 from box cox in response transformation 
mod9_<- lm(I(O3**.25)~temp+humidity+ibh, data= ozone)


qqnorm(mod9_$residuals)
qqline(as.data.frame(mod9_$residuals))
```

Using the value of .25 from the box cox, we apply this value to the response and based on the new qqplot the problem of long tail appears corrected. 

###question 9.8

####(a)
```{r}
#a
plot(cars$speed, cars$dist, xlab = "speed", ylab = "distance", main = "distance vs speed")
```

####(b)
```{r}
#b
plot(cars$speed, cars$dist, xlab = "speed", ylab = "distance", main = "distance vs speed")
mod8<- lm(dist~speed, data = cars)
abline(mod8, col = "red")
summary(mod8)
```

####(c)
```{r}
#c
plot(cars$speed, cars$dist, xlab = "speed", ylab = "distance", main = "distance vs speed")

mod8q<- lm(dist~poly(speed, 2), data = cars)
summary(mod8q)
abline(mod8, col = "red")
lines(cars$speed, mod8q$fitted.values, col= "blue")
legend("topleft", legend = c("least", "quadratic"),col=c("red", "blue"), lty=c(1,1))
```

####(d)
```{r}
#d
plot(cars$speed, cars$dist, xlab = "speed", ylab = "distance", main = "distance vs speed")
mod8s<- lm(I(sqrt(dist))~speed, data = cars)
summary(mod8s)
abline(mod8, col = "red")
lines(cars$speed, mod8q$fitted.values, col= "blue")
abline(mod8s, col = "green")
legend("topleft", legend = c("least", "quadratic", "sqrt"),col=c("red", "blue", "green"), lty=c(1,1, 1))
```

####(e)
```{r}
#e

plot(cars$speed, cars$dist, xlab = "speed", ylab = "distance", main = "distance vs speed")
abline(mod8, col = "red")
lines(cars$speed, mod8q$fitted.values, col= "blue")
abline(mod8s, col = "green")
lines(smooth.spline(cars$speed, cars$dist), col = "black" , lty = 5)
legend("topleft", legend = c("least", "quadratic", "sqrt", "spline"),col=c("red", "blue", "green", "black"), lty=c(1,1, 1, 5))

```

The spline appears to be a good fit. I think the spline and the quadratic are great fits to the data. Then followed by the least square method without any transformation(redline). From the plot, the sqrt of the response appears not to be good. 
